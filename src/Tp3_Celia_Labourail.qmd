---
title: ''
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
fontsize: 12pt
geometry: top=1.5cm, bottom=1.5cm, left=2cm, right=2cm
header-includes:
- \usepackage{graphicx}
- \usepackage{fancyhdr}
execute:
    echo: false
\centering

    \centering
    \includegraphics[width=\linewidth]{../figures/Logo.png}
\end{minipage}
\hspace{1cm}
\begin{minipage}[c]{6cm}
    \centering
    \includegraphics[width=\linewidth]{../figures/ssd_logo_couleur_noir_variante_biostats.png}
\end{minipage}

\vspace{0.5cm}

{\scshape\LARGE Université de Montpellier \par}
\vspace{1cm}
{\scshape\Large Apprentissage statistique \par}
\vspace{0.5cm}
\rule{\linewidth}{0.5 mm} \\[0.4 cm]
{\huge\bfseries  TP 3: SVM \par}
\rule{\linewidth}{0.5 mm} \\[1.5 cm]

% Élève et encadrante
\begin{minipage}{0.5\textwidth}
\begin{flushleft} \large
\emph{\textbf{Élève :}}\\
Labourail Célia
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{\textbf{Encadrante :}} \\
B.Bensaid
\end{flushright}
\end{minipage}

\vspace*{\fill}

% Date
\begin{center}
{\today}
\end{center}

\end{titlepage}

\section*{Introduction}

L'objectif de ce TP est d'explorer les \textbf{Machines à Vecteurs de Support (SVM)} sur différents jeux de données afin de comparer les performances selon le noyau utilisé, l'influence du bruit et l'effet d'une réduction de dimension (PCA).  

Les SVM sont efficaces pour la classification supervisée, même lorsque les données ne sont pas linéairement séparables, grâce aux noyaux non linéaires. Ce TP comprend : 
\begin{itemize}
    \item Étude sur un jeu de données synthétique (2 gaussiennes),
    \item Étude sur le jeu de données Iris,
    \item Évaluation de l'effet de variables de nuisance,
    \item Réduction de dimension avec PCA,
    \item Une application sur la reconnaissance de visages.
\end{itemize}

\section*{Méthodologie}

\begin{itemize}
    \item Standardisation des données avant entraînement.
    \item SVM entraînés avec différents noyaux : linéaire et polynomial.
    \item Validation croisée pour sélectionner le meilleur paramètre de régularisation \(C\) et d'autres hyperparamètres.
    \item Évaluation par score sur ensemble d'entraînement et de test.
    \item Visualisation des frontières de décision et projections PCA pour interprétation.
\end{itemize}

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC

from svm_source import *
from sklearn import svm
from sklearn import datasets
from sklearn.utils import shuffle
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
from time import time

scaler = StandardScaler()

import warnings
warnings.filterwarnings("ignore")

plt.style.use('ggplot')
```

```{python}
###############################################################################
#               Toy dataset : 2 gaussians
###############################################################################

n1 = 200
n2 = 200
mu1 = [1., 1.]
mu2 = [-1./2, -1./2]
sigma1 = [0.9, 0.9]
sigma2 = [0.9, 0.9]
X1, y1 = rand_bi_gauss(n1, n2, mu1, mu2, sigma1, sigma2)

plt.show()
plt.close("all")
plt.ion()
plt.figure(1, figsize=(15, 5))
plt.title('First data set')
plot_2d(X1, y1)



X_train = X1[::2]
Y_train = y1[::2].astype(int)
X_test = X1[1::2]
Y_test = y1[1::2].astype(int)

# fit the model with linear kernel
clf = SVC(kernel='linear')
clf.fit(X_train, Y_train)

# predict labels for the test data base
y_pred = clf.predict(X_test)

# check your score
score = clf.score(X_test, Y_test)
print('Score : %s' % score)

# display the frontiere
def f(xx):
    """Classifier: needed to avoid warning due to shape issues"""
    return clf.predict(xx.reshape(1, -1))

plt.figure()
frontiere(f, X_train, Y_train, w=None, step=50, alpha_choice=1)

# Same procedure but with a grid search
parameters = {'kernel': ['linear'], 'C': list(np.linspace(0.001, 3, 21))}
clf2 = SVC()
clf_grid = GridSearchCV(clf2, parameters, n_jobs=1)
clf_grid.fit(X_train, Y_train)


# check your score
print(clf_grid.best_params_)
print('Score : %s' % clf_grid.score(X_test, Y_test))

def f_grid(xx):
    """Classifier: needed to avoid warning due to shape issues"""
    return clf_grid.predict(xx.reshape(1, -1))

# display the frontiere
plt.figure()
frontiere(f_grid, X_train, Y_train, w=None, step=50, alpha_choice=1)

```

section*{Résultats}

\subsection*{1. Jeu de données synthétique (2 gaussiennes)}

\begin{itemize}
    \item Score SVM linéaire : \(0.905\)  
    \item Visualisation de la frontière de décision : séparation correcte des deux classes.
\end{itemize}

\textbf{Interprétation :} Le SVM linéaire sépare correctement les deux distributions. Même un modèle simple peut être efficace sur des données bien séparées.
```{python}
###############################################################################
#               Iris Dataset
###############################################################################



iris = datasets.load_iris()
X = iris.data
X = scaler.fit_transform(X)
y = iris.target
X = X[y != 0, :2]
y = y[y != 0]

# split train test
X, y = shuffle(X, y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
###############################################################################
# fit the model with linear vs polynomial kernel
###############################################################################
```


```{python}
#Q1 Linear kernel



# fit the model
parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 200))}

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

clf_linear = GridSearchCV(SVC(), parameters, cv=5)

# Entraînement
clf_linear.fit(X_train, y_train)

# compute the score
print('Generalization score for linear kernel: %s, %s' %
      (clf_linear.score(X_train, y_train),
       clf_linear.score(X_test, y_test)))



```


```{python}
# Q2 polynomial kernel


Cs = list(np.logspace(-3, 3, 5))
gammas = 10. ** np.arange(1, 2)
degrees = np.r_[1, 2, 3]

parameters = {'kernel': ['poly'], 'C': Cs, 'gamma': gammas, 'degree': degrees}

clf_poly = GridSearchCV(SVC(), parameters, cv=5)
clf_poly.fit(X_train, y_train)

print(clf_poly.best_params_)
print('Generalization score for polynomial kernel: %s, %s' %
      (clf_poly.score(X_train, y_train),
       clf_poly.score(X_test, y_test)))


```

```{python}
# display your results using frontiere (svm_source.py)

# Prédiction avec le modèle linear
def f_linear(xx):
    return clf_linear.predict(np.array(xx).reshape(1, -1))  # reshape 1 échantillon

# Prédiction avec le modèle polynomial
def f_poly(xx):
    return clf_poly.predict(np.array(xx).reshape(1, -1))
plt.ion()
plt.figure(figsize=(15, 5))
plt.subplot(131)
plot_2d(X, y)
plt.title("iris dataset")

plt.subplot(132)
frontiere(f_linear, X, y)
plt.title("linear kernel")

plt.subplot(133)
frontiere(f_poly, X, y)

plt.title("polynomial kernel")
plt.tight_layout()
plt.draw()


```

\subsection*{2. Jeu de données Iris}

\begin{itemize}
    \item Noyau linéaire : score raisonabe proche de 1   
    \item Noyau polynomial : score entraînement et test similaire, paramètres optimaux choisis par validation croisée. polynome de degrés 1 
\end{itemize}

\textbf{Interprétation :} Le noyau linéaire fonctionne très bien sur Iris, mais le noyau polynomial peut légèrement améliorer la performance sur des classes plus complexes si degrés supérieur a 1 . La généralisation reste satisfaisante.
```{python}
###############################################################################
#               SVM GUI
###############################################################################

# please open a terminal and run python svm_gui.py
# Then, play with the applet : generate various datasets and observe the
# different classifiers you can obtain by varying the kernel


```

![](../figures/svm_gui_capture.png)
*Figure : Capture d'écran de l'interface graphique SVM (Scikit-learn Libsvm GUI)*
D
ans l’exemple réalisé avec le SVM GUI, le jeu de données est fortement déséquilibré et le paramètre de régularisation C
C est petit. On observe alors que le modèle a tendance à favoriser la classe majoritaire, au détriment de la classe minoritaire. Le score globalS peut paraître correct, mais il masque en réalité une mauvaise classification des échantillons minoritaires.

Si l’on diminue encore C
C, le SVM devient plus souple, la marge est pluslarge et le modèle tolère davantage les erreurs sur l’ensemble d’entraînement. Cela augmente encore le risque de sous‑apprentissage, accentuant la tendance à prédire principalement la classe majoritaire.

En résumé, sur un jeu déséquilibré, un C

C trop petit rend le modèle trop généraliste, ce qui réduit sa capacité à détecter correctement la classe minoritaire. Il est donc essentiel de choisir un C
C adapté ou d’utiliser des technique de rééquilibrage pour obtenir une classification fiable.
```{python}
#%%
###############################################################################
#               Face Recognition Task
###############################################################################

"""
The dataset used in this example is a preprocessed excerpt
of the "Labeled Faces in the Wild", aka LFW_:

  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)

  _LFW: http://vis-www.cs.umass.edu/lfw/
"""

####################################################################
# Download the data and unzip; then load it as numpy arrays
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4,
                              color=True, funneled=False, slice_=None,
                              download_if_missing=True)
# data_home='.'

# introspect the images arrays to find the shapes (for plotting)
images = lfw_people.images
n_samples, h, w, n_colors = images.shape

# the label to predict is the id of the person
target_names = lfw_people.target_names.tolist()

####################################################################
# Pick a pair to classify such as
names = ['Tony Blair', 'Colin Powell']
# names = ['Donald Rumsfeld', 'Colin Powell']

idx0 = (lfw_people.target == target_names.index(names[0]))
idx1 = (lfw_people.target == target_names.index(names[1]))
images = np.r_[images[idx0], images[idx1]]
n_samples = images.shape[0]
y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(int)

# plot a sample set of the data
plot_gallery(images, np.arange(12))
plt.show()


```


```{python}
####################################################################
# Extract features

# features using only illuminations
X = (np.mean(images, axis=3)).reshape(n_samples, -1)

# # or compute features using colors (3 times more features)
# X = images.copy().reshape(n_samples, -1)

# Scale features
X -= np.mean(X, axis=0)
X /= np.std(X, axis=0)



```
```{python}
####################################################################
# Split data into a half training and half test set
# X_train, X_test, y_train, y_test, images_train, images_test = \
#    train_test_split(X, y, images, test_size=0.5, random_state=0)
# X_train, X_test, y_train, y_test = \
#    train_test_split(X, y, test_size=0.5, random_state=0)

indices = np.random.permutation(X.shape[0])
train_idx, test_idx = indices[:X.shape[0] // 2], indices[X.shape[0] // 2:]
X_train, X_test = X[train_idx, :], X[test_idx, :]
y_train, y_test = y[train_idx], y[test_idx]
images_train, images_test = images[
    train_idx, :, :, :], images[test_idx, :, :, :]

####################################################################
# Quantitative evaluation of the model quality on the test set


```
```{python}
# Q4
print("--- Linear kernel ---")
print("Fitting the classifier to the training set")
t0 = time()

# fit a classifier (linear) and test all the Cs
Cs = 10. ** np.arange(-5, 6)
scores = []

for C in Cs:
    clf_temp = SVC(kernel='linear', C=C)   # créer un SVM linéaire avec ce C
    clf_temp.fit(X_train, y_train)         # entraîner sur le train
    scores.append(clf_temp.score(X_test, y_test))  # ajouter la précision sur le test

ind = np.argmax(scores)
print("Best C: {}".format(Cs[ind]))

plt.figure()
plt.plot(Cs, scores, marker='o')
plt.xlabel("Paramètres de régularisation C")
plt.ylabel("Scores d'apprentissage")
plt.xscale("log")
plt.tight_layout()
plt.show()
print("Best score: {}".format(np.max(scores)))



print("Predicting the people names on the testing set")
t0 = time()

# predict labels for the X_test images with the best classifier
clf = SVC(kernel='linear', C=Cs[ind])
clf.fit(X_train, y_train)

print("done in %0.3fs" % (time() - t0))
# The chance level is the accuracy that will be reached when constantly predicting the majority class.
print("Chance level : %s" % max(np.mean(y), 1. - np.mean(y)))
print("Accuracy : %s" % clf.score(X_test, y_test))



```


```{python}
####################################################################
# Qualitative evaluation of the predictions using matplotlib

prediction_titles = [title(y_pred[i], y_test[i], names)
                     for i in range(len(y_test))]

plot_gallery(images_test, prediction_titles)
plt.show()

####################################################################
# Look at the coefficients
plt.figure()
plt.imshow(np.reshape(clf.coef_, (h, w)))
plt.show()


```

\subsection*{3. Reconnaissance de visages}

\begin{itemize}
    \item Dataset : LFW, deux personnes sélectionnées.
    \item Features : moyenne des couleurs par pixel.
    \item SVM linéaire : score test ~\(0.95\)
\end{itemize}

\textbf{Interprétation :} Les SVM linéaires peuvent reconnaître efficacement les visages lorsqu’ils sont correctement prétraités. Les coefficients du SVM mettent en évidence les zones discriminantes du visage.




```{python}
# Q5

def run_svm_cv(_X, _y):
    _indices = np.random.permutation(_X.shape[0])
    _train_idx, _test_idx = _indices[:_X.shape[0] // 2], _indices[_X.shape[0] // 2:]
    _X_train, _X_test = _X[_train_idx, :], _X[_test_idx, :]
    _y_train, _y_test = _y[_train_idx], _y[_test_idx]

    _parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 5))}
    _svr = svm.SVC()
    _clf_linear = GridSearchCV(_svr, _parameters)
    _clf_linear.fit(_X_train, _y_train)     

    print('Generalization score for linear kernel: %s, %s \n' %
          (_clf_linear.score(_X_train, _y_train), _clf_linear.score(_X_test, _y_test)))




print("Score sans variable de nuisance")
# TODO ... use run_svm_cv on data
run_svm_cv(X, y)



print("Score avec variable de nuisance")
n_samples, n_features = X.shape
sigma = 1

# On rajoute des variables de nuisances
noise = sigma * np.random.randn(n_samples, 300)  # 300 variables aléatoires

# Ajouter les variables de nuisance à X
X_noisy = np.hstack((X, noise))

# Mélanger aléatoirement les colonnes
cols = np.random.permutation(X_noisy.shape[1])
X_noisy = X_noisy[:, cols]

# TODO ... use run_svm_cv on noisy data
run_svm_cv(X_noisy, y)


```

\subsection*{4. Effet des variables de nuisance}

\begin{itemize}
    \item Score test SVM linéaire avec variables de nuisance : \(0.8947\)  
    \item Score sans variables de nuisance : \(0.9158\)
\end{itemize}

\textbf{Interprétation :} L’ajout de variables aléatoires diminue légèrement les performances. Le SVM reste robuste grâce à la régularisation.


```{python}
# Q6
print("Score apres reduction de dimension")

n_components = 10  # jouer avec ce parametre
pca = PCA(n_components=n_components, svd_solver='randomized').fit(X_noisy)

# TODO ... projection des données et appel de run_svm_cv
X_pca = pca.transform(X_noisy)  # projeter les données sur les n_components premiers axes          
score = run_svm_cv(X_pca, y)   # exécuter le SVM sur les données réduites


# %%
plt.figure(figsize=(8,6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=plt.cm.Set1, edgecolor="k", s=40)
plt.xlabel("Composante principale 1")
plt.ylabel("Composante principale 2")
plt.title(f"Projection PCA (2 premières composantes) - n_components={n_components}")
plt.tight_layout()
plt.show()

plt.figure()
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.xlabel("Nombre de composantes")
plt.ylabel("Variance expliquée cumulée")
plt.title("PCA - Variance expliquée")
plt.grid(True)
plt.show()
```
\subsection*{5. Réduction de dimension (PCA)}

\begin{itemize}
    \item Projection des données sur les deux premières composantes principales.
    \item Score après PCA (avec \(n\_components=5\)) : ~\(0.91\)
\end{itemize}

\textbf{Interprétation :} La PCA permet de conserver la structure des données tout en réduisant la dimensionnalité. Utile pour accélérer l’apprentissage sur des datasets volumineux ou bruités.



\section*{Conclusion}

\begin{itemize}
    \item Les SVM sont très performants pour la classification supervisée sur des jeux de données simples et moyennement complexes.
    \item Le choix du noyau et des hyperparamètres influence fortement la performance.
    \item Les SVM sont robustes face aux variables de nuisance grâce à la régularisation.
    \item La réduction de dimension via PCA est efficace pour accélérer l’apprentissage tout en conservant l’information pertinente.
    \item Les SVM peuvent être appliqués à des tâches réelles, comme la reconnaissance faciale, avec de bons résultats.
\end{itemize}

On remarque que dans le code fourni, certaines évaluations du SVM utilisent directement les données d’entraînement pour calculer la performance, par exemple lors du calcul des scores avec clf_linear.score(X_train, y_train). Cela signifie que le modèle "connaît" déjà ces données, ce qui peut conduire à des scores optimistes.

Idéalement, l’évaluation devrait se faire uniquement sur des données de test séparées qui n’ont pas été utilisées pour l’entraînement, afin d’obtenir une estimation réaliste de la performance du modèle sur de nouvelles données.