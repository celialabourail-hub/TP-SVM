{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: ''\n",
        "date: \"`r Sys.Date()`\"\n",
        "output:\n",
        "  pdf_document:\n",
        "    latex_engine: xelatex\n",
        "  word_document: default\n",
        "fontsize: 12pt\n",
        "geometry: top=1.5cm, bottom=1.5cm, left=2cm, right=2cm\n",
        "header-includes:\n",
        "- \\usepackage{graphicx}\n",
        "- \\usepackage{fancyhdr}\n",
        "- \\usepackage{wallpaper}\n",
        "- \\usepackage{caption}\n",
        "- \\usepackage{subcaption}\n",
        "---"
      ],
      "id": "33e050f6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\begin{titlepage}\n",
        "\\centering\n",
        "\n",
        "\\begin{minipage}[c]{6cm}\n",
        "    \\centering\n",
        "    \\includegraphics[width=\\linewidth]{Logo.png}\n",
        "\\end{minipage}\n",
        "\\hspace{1cm}\n",
        "\\begin{minipage}[c]{6cm}\n",
        "    \\centering\n",
        "    \\includegraphics[width=\\linewidth]{ssd_logo_couleur_noir_variante_biostats.png}\n",
        "\\end{minipage}\n",
        "\n",
        "\\vspace{0.5cm}\n",
        "\n",
        "{\\scshape\\LARGE Université de Montpellier \\par}\n",
        "\\vspace{1cm}\n",
        "{\\scshape\\Large Apprentissage statistique \\par}\n",
        "\\vspace{0.5cm}\n",
        "\\rule{\\linewidth}{0.5 mm} \\\\[0.4 cm]\n",
        "{\\huge\\bfseries  TP 3: SVM \\par}\n",
        "\\rule{\\linewidth}{0.5 mm} \\\\[1.5 cm]\n",
        "\n",
        "% Élève et encadrante\n",
        "\\begin{minipage}{0.5\\textwidth}\n",
        "\\begin{flushleft} \\large\n",
        "\\emph{\\textbf{Élève :}}\\\\\n",
        "Labourail Célia\n",
        "\\end{flushleft}\n",
        "\\end{minipage}\n",
        "~\n",
        "\\begin{minipage}{0.4\\textwidth}\n",
        "\\begin{flushright} \\large\n",
        "\\emph{\\textbf{Encadrante :}} \\\\\n",
        "B.Bensaid\n",
        "\\end{flushright}\n",
        "\\end{minipage}\n",
        "\n",
        "\\vspace*{\\fill}\n",
        "\n",
        "% Date\n",
        "\\begin{center}\n",
        "{\\today}\n",
        "\\end{center}\n",
        "\n",
        "\\end{titlepage}\n",
        "\n",
        "# Introduction\n"
      ],
      "id": "639fe324"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from svm_source import *\n",
        "from sklearn import svm\n",
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.decomposition import PCA\n",
        "from time import time\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "plt.style.use('ggplot')"
      ],
      "id": "5d38127b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "###############################################################################\n",
        "#               Toy dataset : 2 gaussians\n",
        "###############################################################################\n",
        "\n",
        "n1 = 200\n",
        "n2 = 200\n",
        "mu1 = [1., 1.]\n",
        "mu2 = [-1./2, -1./2]\n",
        "sigma1 = [0.9, 0.9]\n",
        "sigma2 = [0.9, 0.9]\n",
        "X1, y1 = rand_bi_gauss(n1, n2, mu1, mu2, sigma1, sigma2)\n",
        "\n",
        "plt.show()\n",
        "plt.close(\"all\")\n",
        "plt.ion()\n",
        "plt.figure(1, figsize=(15, 5))\n",
        "plt.title('First data set')\n",
        "plot_2d(X1, y1)\n",
        "\n",
        "X_train = X1[::2]\n",
        "Y_train = y1[::2].astype(int)\n",
        "X_test = X1[1::2]\n",
        "Y_test = y1[1::2].astype(int)\n",
        "\n",
        "# fit the model with linear kernel\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train, Y_train)\n",
        "\n",
        "# predict labels for the test data base\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# check your score\n",
        "score = clf.score(X_test, Y_test)\n",
        "print('Score : %s' % score)\n",
        "\n",
        "# display the frontiere\n",
        "def f(xx):\n",
        "    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n",
        "    return clf.predict(xx.reshape(1, -1))\n",
        "\n",
        "plt.figure()\n",
        "frontiere(f, X_train, Y_train, w=None, step=50, alpha_choice=1)\n",
        "\n",
        "# Same procedure but with a grid search\n",
        "parameters = {'kernel': ['linear'], 'C': list(np.linspace(0.001, 3, 21))}\n",
        "clf2 = SVC()\n",
        "clf_grid = GridSearchCV(clf2, parameters, n_jobs=1)\n",
        "clf_grid.fit(X_train, Y_train)\n",
        "\n",
        "# check your score\n",
        "print(clf_grid.best_params_)\n",
        "print('Score : %s' % clf_grid.score(X_test, Y_test))\n",
        "\n",
        "def f_grid(xx):\n",
        "    \"\"\"Classifier: needed to avoid warning due to shape issues\"\"\"\n",
        "    return clf_grid.predict(xx.reshape(1, -1))\n",
        "\n",
        "# display the frontiere\n",
        "plt.figure()\n",
        "frontiere(f_grid, X_train, Y_train, w=None, step=50, alpha_choice=1)"
      ],
      "id": "321d04f4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "###############################################################################\n",
        "#               Iris Dataset\n",
        "###############################################################################\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "X = scaler.fit_transform(X)\n",
        "y = iris.target\n",
        "X = X[y != 0, :2]\n",
        "y = y[y != 0]\n",
        "\n",
        "# split train test\n",
        "X, y = shuffle(X, y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "###############################################################################\n",
        "# fit the model with linear vs polynomial kernel\n",
        "###############################################################################"
      ],
      "id": "e5b23ebe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Q1 Linear kernel\n",
        "\n",
        "# fit the model\n",
        "parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 200))}\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "clf_linear = GridSearchCV(SVC(), parameters, cv=5)\n",
        "\n",
        "# Entraînement\n",
        "clf_linear.fit(X_train, y_train)\n",
        "\n",
        "# compute the score\n",
        "print('Generalization score for linear kernel: %s, %s' %\n",
        "      (clf_linear.score(X_train, y_train),\n",
        "       clf_linear.score(X_test, y_test)))"
      ],
      "id": "38f93a05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Q2 polynomial kernel\n",
        "Cs = list(np.logspace(-3, 3, 5))\n",
        "gammas = 10. ** np.arange(1, 2)\n",
        "degrees = np.r_[1, 2, 3]\n",
        "\n",
        "parameters = {'kernel': ['poly'], 'C': Cs, 'gamma': gammas, 'degree': degrees}\n",
        "\n",
        "clf_poly = GridSearchCV(SVC(), parameters, cv=5)\n",
        "clf_poly.fit(X_train, y_train)\n",
        "\n",
        "print(clf_poly.best_params_)\n",
        "print('Generalization score for polynomial kernel: %s, %s' %\n",
        "      (clf_poly.score(X_train, y_train),\n",
        "       clf_poly.score(X_test, y_test)))"
      ],
      "id": "daaf6489",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# display your results using frontiere (svm_source.py)\n",
        "\n",
        "# Prédiction avec le modèle linear\n",
        "def f_linear(xx):\n",
        "    return clf_linear.predict(np.array(xx).reshape(1, -1))  # reshape 1 échantillon\n",
        "\n",
        "# Prédiction avec le modèle polynomial\n",
        "def f_poly(xx):\n",
        "    return clf_poly.predict(np.array(xx).reshape(1, -1))\n",
        "plt.ion()\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(131)\n",
        "plot_2d(X, y)\n",
        "plt.title(\"iris dataset\")\n",
        "\n",
        "plt.subplot(132)\n",
        "frontiere(f_linear, X, y)\n",
        "plt.title(\"linear kernel\")\n",
        "\n",
        "plt.subplot(133)\n",
        "frontiere(f_poly, X, y)\n",
        "\n",
        "plt.title(\"polynomial kernel\")\n",
        "plt.tight_layout()\n",
        "plt.draw()"
      ],
      "id": "5355b656",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "###############################################################################\n",
        "#               SVM GUI\n",
        "###############################################################################\n",
        "\n",
        "# please open a terminal and run python svm_gui.py\n",
        "# Then, play with the applet : generate various datasets and observe the\n",
        "# different classifiers you can obtain by varying the kernel\n"
      ],
      "id": "177cf274",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#%%\n",
        "###############################################################################\n",
        "#               Face Recognition Task\n",
        "###############################################################################\n",
        "\"\"\"\n",
        "The dataset used in this example is a preprocessed excerpt\n",
        "of the \"Labeled Faces in the Wild\", aka LFW_:\n",
        "\n",
        "  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n",
        "\n",
        "  _LFW: http://vis-www.cs.umass.edu/lfw/\n",
        "\"\"\"\n",
        "\n",
        "####################################################################\n",
        "# Download the data and unzip; then load it as numpy arrays\n",
        "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4,\n",
        "                              color=True, funneled=False, slice_=None,\n",
        "                              download_if_missing=True)\n",
        "# data_home='.'\n",
        "\n",
        "# introspect the images arrays to find the shapes (for plotting)\n",
        "images = lfw_people.images\n",
        "n_samples, h, w, n_colors = images.shape\n",
        "\n",
        "# the label to predict is the id of the person\n",
        "target_names = lfw_people.target_names.tolist()\n",
        "\n",
        "####################################################################\n",
        "# Pick a pair to classify such as\n",
        "names = ['Tony Blair', 'Colin Powell']\n",
        "# names = ['Donald Rumsfeld', 'Colin Powell']\n",
        "\n",
        "idx0 = (lfw_people.target == target_names.index(names[0]))\n",
        "idx1 = (lfw_people.target == target_names.index(names[1]))\n",
        "images = np.r_[images[idx0], images[idx1]]\n",
        "n_samples = images.shape[0]\n",
        "y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(int)\n",
        "\n",
        "# plot a sample set of the data\n",
        "plot_gallery(images, np.arange(12))\n",
        "plt.show()"
      ],
      "id": "b9c2b6eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "####################################################################\n",
        "# Extract features\n",
        "\n",
        "# features using only illuminations\n",
        "X = (np.mean(images, axis=3)).reshape(n_samples, -1)\n",
        "\n",
        "# # or compute features using colors (3 times more features)\n",
        "# X = images.copy().reshape(n_samples, -1)\n",
        "\n",
        "# Scale features\n",
        "X -= np.mean(X, axis=0)\n",
        "X /= np.std(X, axis=0)"
      ],
      "id": "ce67a380",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "####################################################################\n",
        "# Split data into a half training and half test set\n",
        "# X_train, X_test, y_train, y_test, images_train, images_test = \\\n",
        "#    train_test_split(X, y, images, test_size=0.5, random_state=0)\n",
        "# X_train, X_test, y_train, y_test = \\\n",
        "#    train_test_split(X, y, test_size=0.5, random_state=0)\n",
        "\n",
        "indices = np.random.permutation(X.shape[0])\n",
        "train_idx, test_idx = indices[:X.shape[0] // 2], indices[X.shape[0] // 2:]\n",
        "X_train, X_test = X[train_idx, :], X[test_idx, :]\n",
        "y_train, y_test = y[train_idx], y[test_idx]\n",
        "images_train, images_test = images[\n",
        "    train_idx, :, :, :], images[test_idx, :, :, :]\n",
        "\n",
        "####################################################################\n",
        "# Quantitative evaluation of the model quality on the test set"
      ],
      "id": "93abebaf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Q4\n",
        "print(\"--- Linear kernel ---\")\n",
        "print(\"Fitting the classifier to the training set\")\n",
        "t0 = time()\n",
        "\n",
        "# fit a classifier (linear) and test all the Cs\n",
        "Cs = 10. ** np.arange(-5, 6)\n",
        "scores = []\n",
        "\n",
        "for C in Cs:\n",
        "    clf_temp = SVC(kernel='linear', C=C)   # créer un SVM linéaire avec ce C\n",
        "    clf_temp.fit(X_train, y_train)         # entraîner sur le train\n",
        "    scores.append(clf_temp.score(X_test, y_test))  # ajouter la précision sur le test\n",
        "\n",
        "ind = np.argmax(scores)\n",
        "print(\"Best C: {}\".format(Cs[ind]))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(Cs, scores, marker='o')\n",
        "plt.xlabel(\"Paramètres de régularisation C\")\n",
        "plt.ylabel(\"Scores d'apprentissage\")\n",
        "plt.xscale(\"log\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"Best score: {}\".format(np.max(scores)))\n",
        "\n",
        "print(\"Predicting the people names on the testing set\")\n",
        "t0 = time()\n",
        "\n",
        "# predict labels for the X_test images with the best classifier\n",
        "clf = SVC(kernel='linear', C=Cs[ind])\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"done in %0.3fs\" % (time() - t0))\n",
        "# The chance level is the accuracy that will be reached when constantly predicting the majority class.\n",
        "print(\"Chance level : %s\" % max(np.mean(y), 1. - np.mean(y)))\n",
        "print(\"Accuracy : %s\" % clf.score(X_test, y_test))"
      ],
      "id": "e36caca2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "####################################################################\n",
        "# Qualitative evaluation of the predictions using matplotlib\n",
        "\n",
        "prediction_titles = [title(y_pred[i], y_test[i], names)\n",
        "                     for i in range(len(y_test))]\n",
        "\n",
        "plot_gallery(images_test, prediction_titles)\n",
        "plt.show()\n",
        "\n",
        "####################################################################\n",
        "# Look at the coefficients\n",
        "plt.figure()\n",
        "plt.imshow(np.reshape(clf.coef_, (h, w)))\n",
        "plt.show()"
      ],
      "id": "03ffd6e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Q5\n",
        "\n",
        "def run_svm_cv(_X, _y):\n",
        "    _indices = np.random.permutation(_X.shape[0])\n",
        "    _train_idx, _test_idx = _indices[:_X.shape[0] // 2], _indices[_X.shape[0] // 2:]\n",
        "    _X_train, _X_test = _X[_train_idx, :], _X[_test_idx, :]\n",
        "    _y_train, _y_test = _y[_train_idx], _y[_test_idx]\n",
        "\n",
        "    _parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 5))}\n",
        "    _svr = svm.SVC()\n",
        "    _clf_linear = GridSearchCV(_svr, _parameters)\n",
        "    _clf_linear.fit(_X_train, _y_train)     \n",
        "\n",
        "    print('Generalization score for linear kernel: %s, %s \\n' %\n",
        "          (_clf_linear.score(_X_train, _y_train), _clf_linear.score(_X_test, _y_test)))\n",
        "\n",
        "\n",
        "print(\"Score sans variable de nuisance\")\n",
        "# TODO ... use run_svm_cv on data\n",
        "run_svm_cv(X, y)\n",
        "\n",
        "\n",
        "print(\"Score avec variable de nuisance\")\n",
        "n_samples, n_features = X.shape\n",
        "sigma = 1\n",
        "\n",
        "# On rajoute des variables de nuisances\n",
        "noise = sigma * np.random.randn(n_samples, 300)  # 300 variables aléatoires\n",
        "\n",
        "# Ajouter les variables de nuisance à X\n",
        "X_noisy = np.hstack((X, noise))\n",
        "\n",
        "# Mélanger aléatoirement les colonnes\n",
        "cols = np.random.permutation(X_noisy.shape[1])\n",
        "X_noisy = X_noisy[:, cols]\n",
        "\n",
        "# TODO ... use run_svm_cv on noisy data\n",
        "run_svm_cv(X_noisy, y)"
      ],
      "id": "5dd78c3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Q6\n",
        "print(\"Score apres reduction de dimension\")\n",
        "\n",
        "n_components = 5  # jouer avec ce parametre\n",
        "pca = PCA(n_components=n_components, svd_solver='randomized').fit(X_noisy)\n",
        "\n",
        "# TODO ... projection des données et appel de run_svm_cv\n",
        "X_pca = pca.transform(X_noisy)  # projeter les données sur les n_components premiers axes          \n",
        "score = run_svm_cv(X_pca, y)   # exécuter le SVM sur les données réduites\n",
        "print(f\"Score après réduction de dimension : {score:.4f}\")\n",
        "\n",
        "# %%\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=plt.cm.Set1, edgecolor=\"k\", s=40)\n",
        "plt.xlabel(\"Composante principale 1\")\n",
        "plt.ylabel(\"Composante principale 2\")\n",
        "plt.title(f\"Projection PCA (2 premières composantes) - n_components={n_components}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.xlabel(\"Nombre de composantes\")\n",
        "plt.ylabel(\"Variance expliquée cumulée\")\n",
        "plt.title(\"PCA - Variance expliquée\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "97bfb03c",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}